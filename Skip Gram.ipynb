{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IDlU-kLCKDVZ"
   },
   "outputs": [],
   "source": [
    "NAME = \"Jun Park\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW9zL4V6KDVc"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ECD5r2hFKDVd",
    "nbgrader": {
     "checksum": "9a0ec075584699a44c46933457b0a8ba",
     "grade": false,
     "grade_id": "cell-a910b376742d04c0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Lab 6: Skip Gram\n",
    "\n",
    "**Please read the following instructions very carefully**\n",
    "\n",
    "## Working on the assignment / FAQs\n",
    "- **Always use the seed/random_state as *42* wherever applicable** (This is to ensure repeatability in answers, across students and coding environments) \n",
    "- The type of question and the points they carry are indicated in each question cell\n",
    "- To avoid any ambiguity, each question also specifies what *value* must be set. Note that these are dummy values and not the answers\n",
    "- If an autograded question has multiple answers (due to differences in handling NaNs, zeros etc.), all answers will be considered.\n",
    "- You can delete the `raise NotImplementedError()`\n",
    "- **Submitting the assignment** : Download the '.ipynb' file from Colab and upload it to bcourses. Do not delete any outputs from cells before submitting.\n",
    "- That's about it. Happy coding!\n",
    "\n",
    "\n",
    "Available software:\n",
    " - Python's Gensim module: https://radimrehurek.com/gensim/ (install using pip)\n",
    " - Sklearn’s  TSNE module in case you use TSNE to reduce dimension (optional)\n",
    " - Python’s Matplotlib (optional)\n",
    "\n",
    "_Note: The most important hyper parameters of skip-gram/CBOW are vector size and windows size_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "Vsocwry-KDVe",
    "nbgrader": {
     "checksum": "a09a0bf3042da711c4bf843e9b4a4189",
     "grade": false,
     "grade_id": "cell-bf780e597c0216c8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "cd469bd0-612f-4f45-b7bf-4c7f1904cb61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
      "File ‘GoogleNews-vectors-negative300.bin.gz’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!wget -nc https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import gensim\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ZF74G9bDKDVh",
    "nbgrader": {
     "checksum": "47031c66b74746d23ccc5e8369446a4b",
     "grade": false,
     "grade_id": "cell-3f89500615a0096f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### **Q1 (1 point)** \n",
    "Find the cosine similarity between the following word pairs\n",
    "\n",
    "- (France, England)\n",
    "- (smaller, bigger)\n",
    "- (England, London)\n",
    "- (France, Rocket)\n",
    "- (big, bigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oohZVeidGU5X",
    "outputId": "243019c1-1fd3-4a8d-e204-142bcd33ae06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "id": "SZD5ZaMvKDVk",
    "nbgrader": {
     "checksum": "4d52dda406c3d8cd5e37d29755f0fb12",
     "grade": false,
     "grade_id": "cell-fbbe575f8f5a6368",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#Replace 0 with the code / value; Do not delete this cell\n",
    "similarity_pair1 = word_vectors.similarity('France','England')\n",
    "similarity_pair2 = word_vectors.similarity('smaller','bigger')\n",
    "similarity_pair3 = word_vectors.similarity('England','London')\n",
    "similarity_pair4 = word_vectors.similarity('France','Rocket')\n",
    "similarity_pair5 = word_vectors.similarity('big','bigger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "tFUPLSK7KDVp",
    "nbgrader": {
     "checksum": "569aa8b664a41d901bf7b0a5e23e9930",
     "grade": true,
     "grade_id": "cell-929d59ed5d67f618",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "dc590685-10e9-4278-d9d0-725bc1515959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39804944 0.7302272 0.43992856 0.07114174 0.68423855\n"
     ]
    }
   ],
   "source": [
    "#This is an autograded cell, do not edit/delete\n",
    "print(similarity_pair1, similarity_pair2, similarity_pair3, similarity_pair4, similarity_pair5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ZcqpWCjJKDVs",
    "nbgrader": {
     "checksum": "a7f270405ddf9ecbffde36e6c096b818",
     "grade": false,
     "grade_id": "cell-ccd6618b4fac3715",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### **Q2 (1 point)** \n",
    "Write an expression to extract the vector representations of the words: \n",
    "\n",
    "- France\n",
    "- England\n",
    "- smaller\n",
    "- bigger\n",
    "- rocket\n",
    "- big\n",
    "\n",
    "Get only the first 5 elements for each vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "id": "6pzKlLyjKDVt",
    "nbgrader": {
     "checksum": "6b3cecb268eb9440c446cd3de984b7f6",
     "grade": false,
     "grade_id": "cell-00f3d05abb28aa23",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#Replace 0 with the code / value to get the first 5 elements of each vector; Do not delete this cell\n",
    "vector_1 = word_vectors['France'][0:5]\n",
    "vector_2 = word_vectors['England'][0:5]\n",
    "vector_3 = word_vectors['smaller'][0:5]\n",
    "vector_4 = word_vectors['bigger'][0:5]\n",
    "vector_5 = word_vectors['rocket'][0:5]\n",
    "vector_6 = word_vectors['big'][0:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "Hkj2ROGTKDVv",
    "nbgrader": {
     "checksum": "401940f859774b3c1ec48338fa15682e",
     "grade": true,
     "grade_id": "cell-6f34229370fa873f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "08319d2c-c331-47e5-8146-94e47279c100"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04858398 0.07861328 0.32421875 0.03491211 0.07714844]\n",
      "[-0.19824219  0.11523438  0.0625     -0.05834961  0.2265625 ]\n",
      "[-0.05004883  0.03417969 -0.0703125   0.17578125  0.00689697]\n",
      "[-0.06542969 -0.09521484 -0.06225586  0.16210938  0.01989746]\n",
      "[-0.03198242  0.27148438 -0.2890625  -0.15429688  0.16894531]\n",
      "[ 0.11132812  0.10595703 -0.07373047  0.18847656  0.07666016]\n"
     ]
    }
   ],
   "source": [
    "#This is an autograded cell, do not edit/delete\n",
    "print(vector_1)\n",
    "print(vector_2)\n",
    "print(vector_3)\n",
    "print(vector_4)\n",
    "print(vector_5)\n",
    "print(vector_6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2UBnMwiXKDVy",
    "nbgrader": {
     "checksum": "ac8b42811c924e7988f17b9dbd3f71ef",
     "grade": false,
     "grade_id": "cell-4ad44071d3785409",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### **Q3 (1 point)** \n",
    "Find the euclidean distances between the word pairs : \n",
    "\n",
    "- (France, England)\n",
    "- (smaller, bigger)\n",
    "- (England, London)\n",
    "- (France, Rocket)\n",
    "- (big, bigger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "id": "zQGd-YVoKDV3",
    "nbgrader": {
     "checksum": "a771483fbb59086604eb84bcc7c7f0ad",
     "grade": false,
     "grade_id": "cell-3aba86afc0ebd8a8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#Replace 0 with the code / value; Do not delete this cell\n",
    "eu_dist1 = np.sqrt(np.sum(np.square(word_vectors['France'] - word_vectors['England'])))\n",
    "eu_dist2 = np.sqrt(np.sum(np.square(word_vectors['smaller'] - word_vectors['bigger'])))\n",
    "eu_dist3 = np.sqrt(np.sum(np.square(word_vectors['England'] - word_vectors['London'])))\n",
    "eu_dist4 = np.sqrt(np.sum(np.square(word_vectors['France'] - word_vectors['Rocket'])))\n",
    "eu_dist5 = np.sqrt(np.sum(np.square(word_vectors['big'] - word_vectors['bigger'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "HsSg0l2UKDV6",
    "nbgrader": {
     "checksum": "17796eb5de342e8f8e841aa137a2c41c",
     "grade": true,
     "grade_id": "cell-15ffa50b82de21ad",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "3a715c69-1557-4f5e-ccd9-ace60bf00a04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0151067\n",
      "1.8618743\n",
      "2.8752837\n",
      "3.892071\n",
      "1.9586496\n"
     ]
    }
   ],
   "source": [
    "#This is an autograded cell, do not edit / delete\n",
    "print(eu_dist1)\n",
    "print(eu_dist2)\n",
    "print(eu_dist3)\n",
    "print(eu_dist4)\n",
    "print(eu_dist5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "XvO2iU7QKDWA",
    "nbgrader": {
     "checksum": "afc0e843c7545e2df83448feda9f28f5",
     "grade": false,
     "grade_id": "cell-7cd8b9b67386376d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### **Q4 (1 point)**\n",
    "Time to dabble with the power of Word2Vec. Find the 2 closest words  for the following conditions:  \n",
    "- (King - Man + Queen)\n",
    "- (bigger - big + small)\n",
    "- (waiting - wait + run)\n",
    "- (Texas + Milwaukee – Wisconsin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "id": "jCxWmA1eKDWB",
    "nbgrader": {
     "checksum": "50ef096feb166865434fe2fca3d41f99",
     "grade": false,
     "grade_id": "cell-b72201968c5fd1ec",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#Replace 0 with the code / value; Do not delete this cell\n",
    "closest1 = word_vectors.most_similar(positive=['King', 'Queen'], negative=['Man'])[0:2]\n",
    "closest2 = word_vectors.most_similar(positive=['bigger', 'small'], negative=['big'])[0:2]\n",
    "closest3 = word_vectors.most_similar(positive=['waiting', 'run'], negative=['wait'])[0:2]\n",
    "closest4 = word_vectors.most_similar(positive=['Texas', 'Milwaukee'], negative=['Wisconsin'])[0:2]\n",
    "closest5 = 0\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "io9elfD8KDWE",
    "nbgrader": {
     "checksum": "f9c5ff502264f29d2632c6387f92686a",
     "grade": true,
     "grade_id": "cell-b69718ab0e1470bc",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "7e8955fa-dee0-4d8d-d336-7083d603aaba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Queen_Elizabeth', 0.5257916450500488), ('monarch', 0.5004087090492249)]\n",
      "[('larger', 0.7402471899986267), ('smaller', 0.732999324798584)]\n",
      "[('running', 0.5654535889625549), ('runs', 0.49640005826950073)]\n",
      "[('Houston', 0.7767744064331055), ('Fort_Worth', 0.7270511388778687)]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#This is an autograded cell, do not edit/delete\n",
    "print(closest1)\n",
    "print(closest2)\n",
    "print(closest3)\n",
    "print(closest4)\n",
    "print(closest5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "erUu4u71KDWJ",
    "nbgrader": {
     "checksum": "6432058d78f4fa52224c48a3b3e71d0d",
     "grade": false,
     "grade_id": "cell-73dca0e2072fef91",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### **Q5 (3 points)**\n",
    "Using the vectors for the words in the Google News dataset, explore the semantic representation of these words through K-means clustering and explain your findings.\n",
    "\n",
    "*Note : Since there are ~3Mil words in the vocabulary, you can downsample it to ~20-30k randomly selected words*\n",
    "\n",
    "**Do not delete the below cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "id": "M3jN02fOKDWK",
    "nbgrader": {
     "checksum": "7ecef46689f11d4d0a6fed72e049235f",
     "grade": true,
     "grade_id": "cell-80b177848b8b0212",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "random_words = np.random.choice(word_vectors.index2word, size = 20000, replace = False)\n",
    "word_vectors = word_vectors[random_words]\n",
    "k_means = KMeans(n_clusters = 10, random_state = 42)\n",
    "k_means.fit(word_vectors)\n",
    "labels = k_means.labels_\n",
    "data = {'word': random_words, 'cluster': labels}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "6bVEbKRH1SLl",
    "outputId": "83a00472-0e43-4da2-d181-28d0b9439bc3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[support_GNET, Mothering_Magazine, Extends_Dea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Lorcan_Cranitch, Sidney_Blumenthal, Trey_Anas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Norjan, Syed_Zahid_Hussain, Rehmani, Kamal_Ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Honeywill, Matic, Sherlock, Duimstra, Louer, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Bisk_Education, Sport_Chek, Earthwatch_Instit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[SilverCity_Riverport_Cinemas, IS_CURRENTLY, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Belvo, Penarth_Vale, Erpingham, asthma_suffer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Wal_i_Musi, Jiangxi_Copper_Ltd., Ricardo_Teix...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Image_Signal_Processor, SJ##, PCIe_x1_slots, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Fang_Wei, distinctively_flavored, Mike_Van_So...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      word\n",
       "cluster                                                   \n",
       "0        [support_GNET, Mothering_Magazine, Extends_Dea...\n",
       "1        [Lorcan_Cranitch, Sidney_Blumenthal, Trey_Anas...\n",
       "2        [Norjan, Syed_Zahid_Hussain, Rehmani, Kamal_Ja...\n",
       "3        [Honeywill, Matic, Sherlock, Duimstra, Louer, ...\n",
       "4        [Bisk_Education, Sport_Chek, Earthwatch_Instit...\n",
       "5        [SilverCity_Riverport_Cinemas, IS_CURRENTLY, P...\n",
       "6        [Belvo, Penarth_Vale, Erpingham, asthma_suffer...\n",
       "7        [Wal_i_Musi, Jiangxi_Copper_Ltd., Ricardo_Teix...\n",
       "8        [Image_Signal_Processor, SJ##, PCIe_x1_slots, ...\n",
       "9        [Fang_Wei, distinctively_flavored, Mike_Van_So..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_by_cluster = df.groupby('cluster').agg(lambda x: list(x))\n",
    "words_by_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12b4g_xX6TXg",
    "outputId": "f36ea473-53a0-48c4-80f5-94bc8daef6ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 represents words relating to the news media industry. \n",
      "There are journalists, newspaper titles, etc. in this cluster.\n"
     ]
    }
   ],
   "source": [
    "## Cluster 0\n",
    "np.array(words_by_cluster.iloc[0,:])\n",
    "print('Cluster 0 represents words relating to the news media industry. \\nThere are journalists, newspaper titles, etc. in this cluster.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcyzjOjT_iI-",
    "outputId": "2d79e751-a4b8-4781-b43e-449061c7f7f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1 represents words relating to the entertainment industry. \n",
      "There are songwriters, actors, etc. in this cluster.\n"
     ]
    }
   ],
   "source": [
    "## Cluster 1\n",
    "np.array(words_by_cluster.iloc[1,:])\n",
    "print('Cluster 1 represents words relating to the entertainment industry. \\nThere are songwriters, actors, etc. in this cluster.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2rZwg8pCLuN",
    "outputId": "80352670-b816-473a-e84e-c76401294bc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 2 represents words relating to education and government. \n",
      "There are names of teachers, scientists, doctors, government officials, etc. in this cluster.\n"
     ]
    }
   ],
   "source": [
    "## Cluster 2\n",
    "np.array(words_by_cluster.iloc[2,:])\n",
    "print('Cluster 2 represents words relating to education and government. \\nThere are names of teachers, scientists, doctors, government officials, etc. in this cluster.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VO26A4biDFZl",
    "outputId": "4562669c-afd6-405b-8fff-0073de590ff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found that each of the clusters contained words related to a certain topic. \n",
      "Although I only went through the first three clusters, I could see that the \n",
      "majority of the words related to a certain topic that could be generalized \n",
      "to all of the words in that cluster.\n"
     ]
    }
   ],
   "source": [
    "## Generalized\n",
    "print('I found that each of the clusters contained words related to a certain topic. \\nAlthough I only went through the first three clusters, I could see that the \\nmajority of the words related to a certain topic that could be generalized \\nto all of the words in that cluster.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "rmdtLoHkKDWR",
    "nbgrader": {
     "checksum": "0467b27a0f59504cbb62b851a002386f",
     "grade": false,
     "grade_id": "cell-5b2a5e8ff6c74323",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### **Q6 (1 point)**\n",
    "What loss function does the skipgram model use and briefly describe what this function is minimizing.\n",
    "\n",
    "**Do not delete the below cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "id": "SyOASYXOKDWS",
    "nbgrader": {
     "checksum": "774aef2c5bf8ef9d92e3489d1cd80390",
     "grade": true,
     "grade_id": "cell-90cc4b2c0ae8e2c2",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# The skipgram model uses categorical cross entropy loss with a softmax activation.\n",
    "# Categorical cross entropy loss as a loss function minimizes\n",
    "# the negative log likelihood of the data belonging to its class summed up\n",
    "# for all classes in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dbpuJx9CKDWV",
    "nbgrader": {
     "checksum": "c14f6069f64cc86ab6e384d28df270d8",
     "grade": false,
     "grade_id": "cell-74a177caaabb5009",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### **Bonus Question (1 point)** \n",
    "Find at least 2 interesting word vec combinations like the ones given in Q4\n",
    "\n",
    "**Do not delete the below cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "pQM8C_T7KDWW",
    "nbgrader": {
     "checksum": "c2d42b5327f4b020c7e1706506dd5ce9",
     "grade": true,
     "grade_id": "cell-7351297993d72e83",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "95d9db34-d737-4b85-849e-5187cfd30837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Cadillac_Escalade', 0.5963462591171265), ('Ford_Expedition', 0.5706534385681152)]\n",
      "[('MLS', 0.640450119972229), ('FIFA', 0.5651741623878479)]\n"
     ]
    }
   ],
   "source": [
    "# Bugatti - sportscar + SUV\n",
    "# If you subtract the sportscar aspect from Bugatti, a high-end sportscar company, you are left with a high-end car company\n",
    "# If you add SUV to that, you would expect to see luxury SUV models, which was correct\n",
    "# The Cadillax Escalade and Ford Expedition are both high-end SUVs in the automotive industry\n",
    "print(word_vectors.most_similar(positive=['Bugatti', 'SUV'], negative=['sportscar'])[0:2])\n",
    "\n",
    "# NBA - basketball + soccer\n",
    "# If you subtract basketball from NBA, a basketball league, you are left with a sports league\n",
    "# If you add soccer to that, you would expect to see examples of soccer leagues, which was correct\n",
    "# The MLS is a soccer league and FIFA is representative of the soccer association\n",
    "print(word_vectors.most_similar(positive=['NBA', 'soccer'], negative=['basketball'])[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c20z0LcaaazN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PARK_JUN_Lab6",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
